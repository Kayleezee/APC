% PREAMBLE
%%%%%%%%%%
%%%%%%%%%%

\documentclass[oneside,a4paper]{scrartcl}

% PACKAGES
%%%%%%%%%%
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{pgf}
\usepackage{placeins}
\usepackage{listings}


% DOCUMENT
%%%%%%%%%%
%%%%%%%%%%

\begin{document}

%%%%%%%
% TITLE
%%%%%%%

\title{Exercise Sheet III}
\subject{Advanced Parallel Computing}
\author{Klaus Naumann \& Christoph Klein}
\maketitle

%%%%%%%
% TABLE OF CONTENTS
%%%%%%%

%\newpage
%\tableofcontents
%\newpage

%%%%%%%
% PART 1 - Reading
%%%%%%%

\section{Review: Cost-Effective Parallel Computing}
The paper "Cost-Effective Parallel Computing" from David A. Wood and 
Mark D. Hill published in February 1995 deals with the authors thesis
that parallel computing can be cost-effective even when speed-ups are
non-linear in comparison to uniprocessor systems. The prerequisite of 
the authors claim is the significant fraction of memory cost compared
to system cost.
\\
In order to take advantage of the efficiency, capacity and bandwidth 
of a large and expensive memory unit several processors are needed.
Cost-effectivity increase especially in cases when speedups exceed 
costups ($\frac{costs(parallel)}{costs(uniprocessor)}$). To further
maximize the benefits of parallel systems software development costs
for parallalizing applications should be taken under control.
\\
In our view the result of the authors work is correct. Especially for
large computational problems parallel systems are far more cost-effective
than uniprocessor systems.

\section{Review: A Survey of Cache Coherence Schemes for Multiprocessor}
The paper "A Survey of Cache Coherence Schemes for Multiprocessor" from
Per Stenstr√∂m released in June 1990 outlines three problems of a shared-
memory multiprocessor: the memory module handling only one request at 
a time, communication contention and latency time which is long because 
of the complexity of the interconnection network. As these problems 
increase memory access time and therefore slow-down the execution, cache 
memory served as a good way to reduce memory acces time. To maintain 
cache coherence in memory the author inspects different possibile 
implemetations: software based, hardware based or a combination of both.
\\
The survey outlines the following hardware based solutions: the 
write-invalidate scheme (when a processor updates a block, all other 
copies set to invalid) and the write-update policy (all copies get updated). 
As software based solutions cache coherence can be realized by indiscriminate 
or selective invalidation or based on timestamps with the aim to avoid 
complex hardware mechanisms. Unfortunately apart from the write-invalidate 
and write-update snoopy protocols none of the other solutions were 
implemented. 
\\
Cache coherence is still a hot topic in parallel systems. Though snoopy
or directoy-based protocols are the most common solution for cache coherence, 
the Dekker's Algorithm is a software based attempt to maintain cache coherence.
    
%%%%%%%
% PART 2 - Experiments
%%%%%%%

\section{Experiment: Shared Counter Performance Analysis}
We performed the experiments for the shared counter performance
analysis on \emph{moore} and on a home computer with an Intel
i5 760 @ 2.80 GHz. You can see our results in figure \ref{plot}, \ref{plotbig},
\ref{plotbigtotal} and \ref{plottotal}. We did two test series: the first
one with $10^5$ and the second with $10^7$ counter increasements. You
see clearly in figure \ref{plot} and \ref{plottotal} that the measured
times are very unstable. We think this happened, because other effects
dominated the time measurements. 

In the other test series (figure \ref{plotbig} and \ref{plotbigtotal}) with
$10^7$ counter increasements, you can see more stable results. The results
in figure \ref{plotbigtotal} look like what we expected, as the total
time does not increase significantly for increasing thread counts
in case of ATOMIC and MUTEX tests.
We expected that, as the communication between threads should be
limited due to latency. Furthermore the relatively low measured total
times for one or two threads is reasonable, because there is less
communication necessary and the compuational intensity of every thread
is nearly zero.

That the time in figure \ref{plotbigtotal} increases for higher thread
counts in case of LOCK\_RMW seems reasonable to us, as this experiment
is executed with a self written mutex, which is based on atomic operations.
Of course this self written mutex is not optimized on hardware level, thus
leads to worse time measurements.

All the aspects we discussed on the behaviour of the total time depending
on the thread count can be transferred to the counter increasements
depending on the thread count. You can see in figure \ref{plotbig} that
we do not see new aspects, which are not already clear.

\begin{figure}
	\centering\input{../src/plot.pgf}
	\caption{Experiments on \emph{moore} and on a local computer at home
	with $10^5$ increasements in total.}
	\label{plot}
\end{figure}

\begin{figure}
	\centering\input{../src/plotbig.pgf}
	\caption{Experiments on \emph{moore} and on a local computer at home
	with $10^7$ increasements in total.}
	\label{plotbig}
\end{figure}

\begin{figure}
	\centering\input{../src/plotbigtotal.pgf}
	\caption{Experiments on \emph{moore} and on a local computer at home
	with $10^7$ increasements in total.}
	\label{plotbigtotal}
\end{figure}
\begin{figure}
	\centering\input{../src/plottotal.pgf}
	\caption{Experiments on \emph{moore} and on a local computer at home
	with $10^5$ increasements in total.}
	\label{plottotal}
\end{figure}
\end{document}
